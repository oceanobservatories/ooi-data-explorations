{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOSTA\n",
    "\n",
    "### Purpose\n",
    "The purpose of this notebook is to calculate the gross range user min/max values for populating QARTOD parameter tables for data streams for OOI - CGSN data streams for the seawater oxygen sensor: the Aanderaa optode. This instrument is deployed on both Coastal and Global Fixed-Depth assets in .\n",
    "\n",
    "This requires the following steps:\n",
    "1. Identify CGSN data streams in OOINet\n",
    "2. Access the relevant data \n",
    "3. Process & combine the data from the different delivery methods and streams into a single dataset\n",
    "4. Add annotations and remove data flagged as bad\n",
    "5. Save the results\n",
    "\n",
    "### Test Parameters\n",
    "\n",
    "| Dataset Name | OOINet Name | Range |\n",
    "| ------------ | ----------- | ----- |\n",
    "| oxygen_concentration_corrected | dissolved_oxygen | 0 - 500 $\\mu$mol/kg |\n",
    "| oxygen_concentration | estimated_oxygen_concentration, dosta_tc_oxygen, dosta_analog_tc_oxygen, dosta_ln_optode_oxygen | 0 - 500 ml/L |\n",
    "| svu_concentration_corrected | dosta_abcdjm_cspp_tc_oxygen | 0 - 500 ml/L |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os, sys, datetime, pytz, re\n",
    "import dateutil.parser as parser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import warnings\n",
    "import gc\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the ```ooinet``` M2M toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/areed/Documents/OOI/reedan88/ooinet/\")\n",
    "from ooinet import M2M\n",
    "from ooinet.utils import convert_time, ntp_seconds_to_datetime, unix_epoch_time\n",
    "from ooinet.Instrument.common import process_file, add_annotation_qc_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import ```ooi_data_explorations``` toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/areed/Documents/OOI/oceanobservatories/ooi-data-explorations/python/\")\n",
    "from ooi_data_explorations.common import get_annotations, get_vocabulary, load_gc_thredds\n",
    "from ooi_data_explorations.combine_data import combine_datasets\n",
    "from ooi_data_explorations.uncabled.process_dosta import dosta_ctdbp_datalogger, dosta_ctdbp_instrument, \\\n",
    "    dosta_datalogger, dosta_wfp\n",
    "from ooi_data_explorations.qartod.qc_processing import identify_blocks, create_annotations, process_gross_range, \\\n",
    "    process_climatology, woa_standard_bins, inputs, ANNO_HEADER, CLM_HEADER, GR_HEADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import plotting and visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../qartod/\")\n",
    "import qartod.plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Identify Data Streams\n",
    "This section is necessary to identify all of the data stream associated with a specific instrument. This can be done by querying UFrame and iteratively walking through all of the API endpoints. The results are saved into a csv file so this step doesn't have to be repeated each time.\n",
    "\n",
    "First, set the instrument to search for using OOI terminology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument = \"DOSTA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query OOINet for Data Streams <br>\n",
    "First check if the datasets have already been downloaded; if not, use the ```M2M.search_datasets``` tool to search the OOINet API and return a table of all of the available datasets for the given instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    datasets = pd.read_csv(\"../data/DOSTA_datasets.csv\")\n",
    "except:\n",
    "    datasets = M2M.search_datasets(instrument=\"DOSTA\", English_names=True)\n",
    "    # Save the datasets\n",
    "    datasets.to_csv(\"../data/DOSTA_datasets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate out the CGSN datasets from the EA and RCA datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgsn = datasets[\"array\"].apply(lambda x: True if x.startswith((\"CP\",\"GA\",\"GI\",\"GP\",\"GS\")) else False)\n",
    "datasets = datasets[cgsn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the ```DOSTAs``` mounted on gliders and AUVs (\"MOAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "moas = datasets[\"array\"].apply(lambda x: True if \"MOAS\" in x else False)\n",
    "datasets = datasets[~moas]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Single Reference Designator\n",
    "The reference designator acts as a key for an instrument located at a specific location. First, select a reference designator (refdes) to request data from OOINet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_designators = sorted(datasets[\"refdes\"])\n",
    "print(\"Number of reference designators: \" + str(len(reference_designators)))\n",
    "for refdes in reference_designators:\n",
    "    print(refdes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a single reference designator (for development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=9\n",
    "refdes = reference_designators[k]\n",
    "print(refdes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensor Vocab\n",
    "The vocab provides information about the instrument model and type, its location (with descriptive names), depth, and manufacturer. Get the vocab for the given reference designator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = M2M.get_vocab(refdes)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensor Deployments\n",
    "Download the deployment information for the selected reference designator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments = M2M.get_deployments(refdes)\n",
    "deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensor Data Streams\n",
    "Next, select the specific data streams for the given reference designator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastreams = M2M.get_datastreams(refdes)\n",
    "datastreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metadata \n",
    "The metadata contains the following important key pieces of data for each reference designator: **method**, **stream**, **particleKey**, and **count**. The method and stream are necessary for identifying and loading the relevant dataset. The particleKey tells us which data variables in the dataset we should be calculating the QARTOD parameters for. The count lets us know which dataset (the recovered instrument, recovered host, or telemetered) contains the most data and likely has the best record to use to calculate the QARTOD tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = M2M.get_metadata(refdes)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensor Parameters\n",
    "Each instrument returns multiple parameters containing a variety of low-level instrument output and metadata. However, we are interested in science-relevant parameters for calculating the relevant QARTOD test limits. We can identify the science parameters based on the preload database, which designates the science parameters with a \"data level\" of L1 or L2. \n",
    "\n",
    "Consequently, we through several steps to identify the relevant parameters. First, we query the preload database with the relevant metadata for a reference designator. Then, we filter the metadata for the science-relevant data streams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_science_parameters(metadata):\n",
    "    \"\"\"This function returns the science parameters for each datastream\"\"\"\n",
    "    \n",
    "    def filter_parameter_ids(pdId, pid_dict):\n",
    "        data_level = pid_dict.get(pdId)\n",
    "        if data_level is not None:\n",
    "            if data_level > 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # Filter the parameters for processed science parameters\n",
    "    data_levels = M2M.get_parameter_data_levels(metadata)\n",
    "    mask = metadata[\"pdId\"].apply(lambda x: filter_parameter_ids(x, data_levels))\n",
    "    metadata = metadata[mask]\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def filter_metadata(metadata):\n",
    "    science_vars = filter_science_parameters(metadata)\n",
    "    # Next, eliminate the optode temperature from the stream\n",
    "    mask = science_vars[\"particleKey\"].apply(lambda x: False if \"temp\" in x else True)\n",
    "    science_vars = science_vars[mask]\n",
    "    science_vars = science_vars.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "    science_vars = science_vars.reset_index()\n",
    "    science_vars = science_vars.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "    science_vars = science_vars.explode(column=\"particleKey\")\n",
    "    return science_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_vars = filter_science_parameters(metadata)\n",
    "science_vars = science_vars.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "science_vars = science_vars.reset_index()\n",
    "science_vars = science_vars.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "science_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data\n",
    "When calculating the QARTOD data tables, we only want to utilize the most complete data record available for a given reference designator. We can identify this by filtering for the largest value under ```count``` which indicates the number of particles in the system for a given dataset. The more particles, the more availabe data. While most of the time this will be the recovered_inst stream, in cases of instrument loss or failure, it may be the record recovered from the mooring host computer (recovered_host) or even data which was telemetered back to shore.\n",
    "\n",
    "First, define a preprocessing function and wrapper to make opening the netCDF datasets easier. This preprocessing function trims the datasets so there are no overlapping time-indices. This allows multiple netCDF files to be opened as a single xarray dataset object while also not having to load the data into memory, significantly speeding up the data load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_overlaps(ds, deployments):\n",
    "    \"\"\"Trim overlapping deployment data (necessary to use xr.open_mfdataset)\"\"\"\n",
    "    # --------------------------------\n",
    "    # Second, get the deployment times\n",
    "    deployments = deployments.sort_values(by=\"deploymentNumber\")\n",
    "    deployments = deployments.set_index(keys=\"deploymentNumber\")\n",
    "    # Shift the start times by (-1) \n",
    "    deployEnd = deployments[\"deployStart\"].shift(-1)\n",
    "    # Find where the deployEnd times are earlier than the deployStart times\n",
    "    mask = deployments[\"deployEnd\"] > deployEnd\n",
    "    # Wherever the deployEnd times occur after the shifted deployStart times, replace those deployEnd times\n",
    "    deployments[\"deployEnd\"][mask] = deployEnd[mask]\n",
    "    deployments[\"deployEnd\"] = deployments[\"deployEnd\"].apply(lambda x: pd.to_datetime(x))\n",
    "    \n",
    "    # ---------------------------------\n",
    "    # With the deployments info, can write a preprocess function to filter \n",
    "    # the data based on the deployment number\n",
    "    depNum = np.unique(ds[\"deployment\"])\n",
    "    deployInfo = deployments.loc[depNum]\n",
    "    deployStart = deployInfo[\"deployStart\"].values[0]\n",
    "    deployEnd = deployInfo[\"deployEnd\"].values[0]\n",
    "    \n",
    "    # Select the dataset data which falls within the specified time range\n",
    "    ds = ds.sel(time=slice(deployStart, deployEnd))\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datalogger(ds):\n",
    "    ds = process_file(ds)\n",
    "    ds = trim_overlaps(ds, deployments)\n",
    "    ds = dosta_datalogger(ds)\n",
    "    gc.collect()\n",
    "    return ds\n",
    "\n",
    "def preprocess_wfp(ds):\n",
    "    ds = process_file(ds)\n",
    "    ds = trim_overlaps(ds, deployments)\n",
    "    ds = dosta_wfp(ds)\n",
    "    gc.collect()\n",
    "    return ds\n",
    "    \n",
    "def preprocess_ctdbp_instrument(ds):\n",
    "    ds = process_file(ds)\n",
    "    ds = trim_overlaps(ds, deployments)\n",
    "    ds = dosta_ctdbp_instrument(ds)\n",
    "    gc.collect()\n",
    "    return ds\n",
    "\n",
    "def preprocess_ctdbp_datalogger(ds):\n",
    "    ds = process_file(ds)\n",
    "    ds = trim_overlaps(ds, deployments)\n",
    "    ds = dosta_ctdbp_datalogger(ds)\n",
    "    gc.collect()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the \"metadata\" datastreams; use only the regular dataset\n",
    "mask = datastreams[\"stream\"].apply(lambda x: False if \"metadata\" in x or \"blank\" in x or \"power\" in x else True)\n",
    "datastreams = datastreams[mask]\n",
    "datastreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Download Data\n",
    "To access data, there are two applicable methods. The first is to download the data and save the netCDF files locally. The second is to access and process the files remotely on the THREDDS server, without having to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the available datasets\n",
    "for index in datastreams.index:\n",
    "    # Get the method and stream\n",
    "    method = datastreams.loc[index][\"method\"]\n",
    "    stream = datastreams.loc[index][\"stream\"]\n",
    "\n",
    "    # Get the URL - first try the goldCopy thredds server\n",
    "    thredds_url = M2M.get_thredds_url(refdes, method, stream, goldCopy=True)\n",
    "\n",
    "    # Get the catalog\n",
    "    catalog = M2M.get_thredds_catalog(thredds_url)\n",
    "\n",
    "    # Clean the catalog\n",
    "    catalog = M2M.clean_catalog(catalog, stream, deployments)\n",
    "    \n",
    "    # Get the links to the THREDDs server and load the data\n",
    "    dodsC = M2M.URLS[\"goldCopy_dodsC\"]\n",
    "    \n",
    "    # Not all datasets have made it into the goldCopy THREDDS - in that case, need to request\n",
    "    # from OOINet\n",
    "    if len(catalog) == 0:\n",
    "        # Get the URL - first try the goldCopy thredds server\n",
    "        thredds_url = M2M.get_thredds_url(refdes, method, stream, goldCopy=False)\n",
    "\n",
    "        # Get the catalog\n",
    "        catalog = M2M.get_thredds_catalog(thredds_url)\n",
    "\n",
    "        # Clean the catalog\n",
    "        catalog = M2M.clean_catalog(catalog, stream, deployments)\n",
    "\n",
    "        # Get the links to the THREDDs server and load the data\n",
    "        dodsC = M2M.URLS[\"dodsC\"]\n",
    "    \n",
    "    # Now load the data\n",
    "    if method == \"telemetered\":\n",
    "        tele_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        if \"ctdbp\" in stream:\n",
    "            with ProgressBar():\n",
    "                tele_data = xr.open_mfdataset(tele_files, preprocess=preprocess_ctdbp_datalogger, parallel=True)\n",
    "        else:\n",
    "            with ProgressBar():\n",
    "                tele_data = xr.open_mfdataset(tele_files, preprocess=preprocess_datalogger, parallel=True)\n",
    "    elif method == \"recovered_host\":\n",
    "        host_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        if \"ctdbp\" in stream:\n",
    "            with ProgressBar():\n",
    "                host_data = xr.open_mfdataset(host_files, preprocess=preprocess_ctdbp_datalogger, parallel=True)\n",
    "        else:\n",
    "            with ProgressBar():\n",
    "                host_data = xr.open_mfdataset(host_files, preprocess=preprocess_datalogger, parallel=True)\n",
    "    elif method == \"recovered_inst\":\n",
    "        inst_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        inst_files = [f for f in inst_files if \"blank\" not in f]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        with ProgressBar():\n",
    "            inst_data = xr.open_mfdataset(inst_files, preprocess=preprocess_ctdbp_instrument, parallel=True) \n",
    "    elif method == \"recovered_wfp\":\n",
    "        wfp_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        wfp_files = [f for f in wfp_files if \"blank\" not in f]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        with ProgressBar():\n",
    "            wfp_data = xr.open_mfdataset(wfp_files, preprocess=preprocess_wfp, parallel=True) \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine the datasets into a single dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combine_datasets(tele_data, host_data, inst_data, None)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean up workspace variables and free up memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_data.close()\n",
    "tele_data.close()\n",
    "inst_data.close()\n",
    "del tele_data, host_data, inst_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Process the DOSTA\n",
    "The DOSTA data needs extensive reprocessing before we can generate the qartod tables. So we download the data, reprocess it, and then will use the reprocessed data to calculate the statistics for the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotations\n",
    "The annotations associated with a specific reference designator may contain relevant information on the performance or reliability of the data for a given dataset. The annotations are downloaded from OOINet as a json and processed into a pandas dataframe. Each annotation may apply to the entire dataset, to a specific stream, or to a specific variable. With the downloaed annotations, we can use the information contained in the ```qcFlag``` column to translate the annotations into QC flags, which can then be used to filter out bad data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotations = M2M.get_annotations(refdes)\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass in the annotations and the dataset to add the annotation ```qcFlag``` values to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_annotation_qc_flag(data, annotations)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the added ```rollup_annotations_qc_results``` values to filter out bad or suspect (```rollup_annotations_qc_results``` value of 3, 4, or 9) data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.where((data.rollup_annotations_qc_results != 3) & (data.rollup_annotations_qc_results != 4), drop=True)\n",
    "data = data.dropna(dim=\"time\", subset=[\"rollup_annotations_qc_results\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit the data to data collected before 2021-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, index = np.unique(data['time'], return_index=True)\n",
    "data = data.isel(time=index)\n",
    "data = data.sel(time=slice('2014-01-01T00:00:00', \"2021-01-01T00:00:00\"))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Gross Range\n",
    "The Gross Range QARTOD test consists of two parameters: a fail range which indicates when the data is bad, and a suspect range which indicates when data is either questionable or interesting. The fail range values are set based upon the instrument/measurement and associated calibration. For example, the conductivity sensors are calibration for measurements between 0 (freshwater) and 9 (highly-saline waters). The suspect range values are calculated based on the mean of the available data $\\pm$3$\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.qartod.gross_range import GrossRange\n",
    "from ooi_data_explorations.qartod.plotting import *\n",
    "from ooi_data_explorations.qartod.qc_processing import format_gross_range, format_climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Parameters & Sensor Ranges\n",
    "Map out the data variables in the data set to the data stream inputs and the associated sensor ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parameters = {\n",
    "    \"oxygen_concentration_corrected\": {\n",
    "        \"inp\": [\"dissolved_oxygen\"],\n",
    "        \"sensor_range\": [0, 500]\n",
    "    }\n",
    "    \"oxygen_concentration\": {\n",
    "        \"inp\": [\"estimated_oxygen_concentration\", \"dosta_tc_oxygen\", \"dosta_analog_tc_oxygen\", \"dosta_ln_optode_oxygen\"],\n",
    "        \"sensor_range\": [0, 500]\n",
    "    }\n",
    "    \"svu_oxygen_concentration\": {\n",
    "        \"inp\": [\"dosta_abcdjm_cspp_tc_oxygen\"],\n",
    "        \"sensor_range\": [0, 500]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the Gross Range Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site, node, sensor = refdes.split(\"-\", 2)\n",
    "gross_range_table = pd.DataFrame()\n",
    "\n",
    "for param in test_parameters:\n",
    "    sensor_range = test_parameters.get(param).get(\"sensor_range\")\n",
    "    inp = test_parameters.get(param).get(\"inp\") \n",
    "    \n",
    "    if param in data.variables:\n",
    "        print(f\"##### Calculating gross range for {param} #####\")\n",
    "        # Check if there is enough data\n",
    "        if len(data[param].dropna(dim=\"time\")) < 100:\n",
    "            user_range = sensor_range\n",
    "            source = \"Not enough data to calculate user range.\"\n",
    "        else:\n",
    "            gross_range = GrossRange(sensor_range[0], sensor_range[1])\n",
    "            gross_range.fit(data, param, check_normality=True)\n",
    "            user_range = [gross_range.suspect_min, gross_range.suspect_max]\n",
    "            source = gross_range.source\n",
    "        # Check which streams have the param in it\n",
    "        for kinp in inp:               \n",
    "            streams = metadata[metadata[\"particleKey\"] == kinp][\"stream\"].unique()\n",
    "            for stream in streams:\n",
    "                qc_dict = format_gross_range(kinp, sensor_range, user_range, site, node, sensor, stream, source)\n",
    "                gross_range_table = gross_range_table.append(qc_dict, ignore_index=True)\n",
    "            \n",
    "        # ------------------ Plot the gross range ------------------\n",
    "        if data[param].time.size > 100000:\n",
    "            try:\n",
    "                subset = sorted(np.random.choice(data.time, 100000, replace=False))\n",
    "                subset_data = data.sel(time=subset)\n",
    "                plot_climatology(subset_data, param, climatology)\n",
    "                del subset, subset_data\n",
    "                gc.collect()\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try:\n",
    "                plot_climatology(data, param, climatology)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = []\n",
    "for stream in streams:\n",
    "    print(\"this works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the stream name and the source comments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_range_table['notes'] = ('User range based on data collected through {}.'.format(\"2021-01-01\"))\n",
    "gross_range_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in gross_range_table.index:\n",
    "    print(gross_range_table.loc[ind][\"qcConfig\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the gross range table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_range_table.to_csv(f\"../results/gross_range/{refdes}.csv\", index=False, columns=GR_HEADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Climatology\n",
    "For the climatology QARTOD test, First, we bin the data by month and take the mean. The binned-montly means are then fit with a 2 cycle harmonic via Ordinary-Least-Squares (OLS) regression. Ranges are calculated based on the 3$\\sigma$ calculated from the OLS-fitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.qartod.climatology import Climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_climatology_table(ds, param, tinp, zinp, sensor_range, depth_bins):\n",
    "    \"\"\"Function which calculates the climatology table based on the \"\"\"\n",
    "    \n",
    "    climatologyTable = pd.DataFrame()\n",
    "    \n",
    "    if depth_bins is None:\n",
    "        # Filter out the data outside the sensor range\n",
    "        m = (ds[param] > sensor_range[0]) & (ds[param] < sensor_range[1]) & (~np.isnan(ds[param]))\n",
    "        param_data = ds[param][m]\n",
    "        \n",
    "        # Fit the climatology for the selected data\n",
    "        pmin, pmax = [0, 0]\n",
    "        \n",
    "        try:\n",
    "            climatology = Climatology()\n",
    "            climatology.fit(param_data)\n",
    "\n",
    "            # Create the depth index\n",
    "            zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "            # Create the monthly bins\n",
    "            tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "            # Calculate the climatology data\n",
    "            vmin = climatology.monthly_fit - climatology.monthly_std*3\n",
    "            vmin = np.floor(vmin*100000)/100000\n",
    "            for vind in vmin.index:\n",
    "                if vmin[vind] < sensor_range[0] or vmin[vind] > sensor_range[1]:\n",
    "                    vmin[vind] = sensor_range[0]\n",
    "            vmax = climatology.monthly_fit + climatology.monthly_std*3\n",
    "            for vind in vmax.index:\n",
    "                if vmax[vind] < sensor_range[0] or vmax[vind] > sensor_range[1]:\n",
    "                    vmax[vind] = sensor_range[1]\n",
    "            vmax = np.floor(vmax*100000)/100000\n",
    "            vdata = pd.Series(data=zip(vmin, vmax), index=vmin.index).apply(lambda x: [v for v in x])\n",
    "            vspan = vdata.values.reshape(1,-1)\n",
    "\n",
    "            # Build the climatology dataframe\n",
    "            climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "        except:\n",
    "            # Here is where to create nans if insufficient data to fit\n",
    "            # Create the depth index\n",
    "            zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "            # Create the monthly bins\n",
    "            tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "            # Create a series filled with nans\n",
    "            vals = []\n",
    "            for i in np.arange(len(tspan)):\n",
    "                vals.append([np.nan, np.nan])\n",
    "            vspan = pd.Series(data=vals, index=tspan).values.reshape(1,-1)\n",
    "\n",
    "            # Add to the data\n",
    "            climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "            \n",
    "        del ds, vspan, tspan, zspan\n",
    "        gc.collect()\n",
    "        \n",
    "    else:        \n",
    "    # Iterate through the depth bins to calculate the climatology for each depth bin\n",
    "        for dbin in depth_bins:\n",
    "            # Get the pressure range to bin from\n",
    "            pmin, pmax = dbin[0], dbin[1]\n",
    "\n",
    "            # Select the data from the pressure range\n",
    "            bin_data = data.where((data[zinp] >= pmin) & (data[zinp] <= pmax), drop=True)\n",
    "\n",
    "            # sort based on time and make sure we have a monotonic dataset\n",
    "            bin_data = bin_data.sortby('time')\n",
    "            _, index = np.unique(bin_data['time'], return_index=True)\n",
    "            bin_data = bin_data.isel(time=index)\n",
    "\n",
    "            # Filter out the data outside the sensor range\n",
    "            m = (bin_data[param] > sensor_range[0]) & (bin_data[param] < sensor_range[1]) & (~np.isnan(bin_data[param]))\n",
    "            param_data = bin_data[param][m]\n",
    "\n",
    "            # Fit the climatology for the selected data\n",
    "            try:\n",
    "                climatology = Climatology()\n",
    "                climatology.fit(param_data)\n",
    "\n",
    "                # Create the depth index\n",
    "                zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "                # Create the monthly bins\n",
    "                tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "                # Calculate the climatology data\n",
    "                vmin = climatology.monthly_fit - climatology.monthly_std*3\n",
    "                vmin = np.floor(vmin*100000)/100000\n",
    "                for vind in vmin.index:\n",
    "                    if vmin[vind] < sensor_range[0] or vmin[vind] > sensor_range[1]:\n",
    "                        vmin[vind] = sensor_range[0]\n",
    "                vmax = climatology.monthly_fit + climatology.monthly_std*3\n",
    "                vmax = np.floor(vmax*100000)/100000\n",
    "                for vind in vmax.index:\n",
    "                    if vmax[vind] < sensor_range[0] or vmax[vind] > sensor_range[1]:\n",
    "                        vmax[vind] = sensor_range[1]\n",
    "                vdata = pd.Series(data=zip(vmin, vmax), index=vmin.index).apply(lambda x: [v for v in x])\n",
    "                vspan = vdata.values.reshape(1,-1)\n",
    "\n",
    "                # Build the climatology dataframe\n",
    "                climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "            except:\n",
    "                # Here is where to create nans if insufficient data to fit\n",
    "                # Create the depth index\n",
    "                zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "                # Create the monthly bins\n",
    "                tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "                # Create a series filled with nans\n",
    "                vals = []\n",
    "                for i in np.arange(len(tspan)):\n",
    "                    vals.append([np.nan, np.nan])\n",
    "                vspan = pd.Series(data=vals, index=tspan).values.reshape(1,-1)\n",
    "\n",
    "                # Add to the data\n",
    "                climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "            del bin_data, vspan, tspan, zspan\n",
    "            gc.collect()\n",
    "    \n",
    "    return climatologyTable, climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the depth bins and filter based on max depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"WFP\" in refdes:\n",
    "    depth_bins = woa_standard_bins()\n",
    "    pmax = data[\"depth\"].max().values\n",
    "    pmin = data[\"depth\"].min().values\n",
    "    mask = (depth_bins[:, 0] < pmax) | ((depth_bins[:, 0] < pmax) & (depth_bins[:, 1] > pmax)) | (depth_bins[:, 1] < pmin)\n",
    "    depth_bins = depth_bins[mask]\n",
    "else:\n",
    "    depth_bins = None\n",
    "depth_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the climatology lookup table\n",
    "climatologyLookup = pd.DataFrame()\n",
    "\n",
    "# Setup the Table Header\n",
    "TBL_HEADER = [\"[1,1]\",\"[2,2]\",\"[3,3]\",\"[4,4]\",\"[5,5]\",\"[6,6]\",\"[7,7]\",\"[8,8]\",\"[9,9]\",\"[10,10]\",\"[11,11]\",\"[12,12]\"]\n",
    "\n",
    "# Set the subsite-node-sensor\n",
    "subsite, node, sensor = refdes.split(\"-\", 2)\n",
    "\n",
    "# Iterate through the parameters\n",
    "for param in test_parameters:\n",
    "    if param in data.variables:\n",
    "        # ----------------- Depth tables ---------------------\n",
    "        # Get the sensor range of the parameter to test\n",
    "        print(f\"##### Calculating climatology for {param} #####\")\n",
    "        sensor_range = test_parameters.get(param).get(\"sensor_range\")\n",
    "        inp = test_parameters.get(param).get(\"inp\")\n",
    "        \n",
    "        # Generate the climatology table with the depth bins\n",
    "        climatologyTable, climatology = make_climatology_table(data, param, \"time\", \"depth\", sensor_range, depth_bins)\n",
    "        \n",
    "         # Get the variance and generate the source\n",
    "        if \"WFP\" in refdes:\n",
    "            source = \"Climatology values are calculated from and applicable to standard depth bins.\"\n",
    "        else:\n",
    "            try:\n",
    "                variance = float(np.round(climatology.regression['variance_explained']*100, 1))\n",
    "            except:\n",
    "                variance = 0.0\n",
    "            source = f\"The variance explained by the climatology mode is {variance}%\"\n",
    "        \n",
    "        # Create the tableName\n",
    "        tableName = f\"{refdes}-{param}.csv\"\n",
    "        \n",
    "        # Save the results\n",
    "        climatologyTable.to_csv(f\"../results/climatology/climatology_tables/{tableName}\", header=TBL_HEADER)\n",
    "        \n",
    "        # ------------------ Lookup tables ------------------\n",
    "        # Check which streams have the param in it\n",
    "        streams = metadata[metadata[\"particleKey\"] == inp][\"stream\"].unique()\n",
    "        for stream in streams:\n",
    "            qc_dict = {\n",
    "                \"subsite\": subsite,\n",
    "                \"node\": node,\n",
    "                \"sensor\": sensor,\n",
    "                \"stream\": stream,\n",
    "                \"parameters\": {\n",
    "                    \"inp\": inp,\n",
    "                    \"tinp\": \"time\",\n",
    "                    \"zinp\": \"depth\",\n",
    "                },\n",
    "                \"climatologyTable\": f\"climatology_tables/{refdes}-{param}.csv\",\n",
    "                \"source\": source,\n",
    "                \"notes\": \"Climatology based on available data through 2021-01-01.\"\n",
    "            }\n",
    "            # Append to the lookup table\n",
    "            climatologyLookup = climatologyLookup.append(qc_dict, ignore_index=True)\n",
    "            \n",
    "        # ------------------ Plot the climatology ------------------\n",
    "        # --------------- Only plot if its NOT a WFP ---------------\n",
    "        if \"WFP\" not in refdes:\n",
    "            if data[param].time.size > 100000:\n",
    "                try:\n",
    "                    subset = sorted(np.random.choice(data.time, 100000, replace=False))\n",
    "                    subset_data = data.sel(time=subset)\n",
    "                    plot_climatology(subset_data, param, climatology)\n",
    "                    del subset, subset_data\n",
    "                    gc.collect()\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                try:\n",
    "                    plot_climatology(data, param, climatology)\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the last climatologyTable for reasonableness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climatologyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the climatologyLookup table that all the entries made it in**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climatologyLookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the climatologyLookup table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climatologyLookup.to_csv(f\"../results/climatology/{refdes}.csv\", index=False, columns=CLM_HEADER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
