{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc74145c",
   "metadata": {},
   "source": [
    "# FLORT\n",
    "\n",
    "### Purpose\n",
    "The purpose of this notebook is to calculate the gross range user min/max values for populating QARTOD parameter tables for data streams for OOI - CGSN data streams for the two-wavelength (FLORD) chlorophyll/fluorescence instrument. The FLORD instruments are deployed both on wire-following-profilers (coastal and global) as well as inductively-via-ctdbps attached to the wire on global surface moorings.\n",
    "\n",
    "This requires the following steps:\n",
    "1. Identify CGSN data streams in OOINet\n",
    "2. Access the relevant data \n",
    "3. Process & combine the data from the different delivery methods and streams into a single dataset\n",
    "4. Add annotations and remove data flagged as bad\n",
    "5. Save the results\n",
    "\n",
    "### Test Parameters and Values\n",
    "\n",
    "\n",
    "| Class | Dataset Name | OOINet Name | Sensor Range |\n",
    "| ----- | ------------ | ----------- | ------------ |\n",
    "| FLORD | | fluorometric_chlorophyll_a | 0 - 30 ug/L |\n",
    "|       | | total_volume_scattering_coefficient | 0 - 5 m$^{-1}$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa5ab99",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d5d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os, sys, datetime, pytz, re\n",
    "import dateutil.parser as parser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import warnings\n",
    "import gc\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7405b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067b044c",
   "metadata": {},
   "source": [
    "#### Import the ```ooinet``` M2M toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OOINet M2M tool\n",
    "sys.path.append(\"/home/areed/Documents/OOI/reedan88/ooinet/\")\n",
    "from ooinet import M2M\n",
    "from ooinet.utils import convert_time, ntp_seconds_to_datetime, unix_epoch_time\n",
    "from ooinet.Instrument.common import process_file, add_annotation_qc_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f152d8",
   "metadata": {},
   "source": [
    "#### Import ```ooi_data_explorations``` toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29dd5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/areed/Documents/OOI/oceanobservatories/ooi-data-explorations/python/\")\n",
    "from ooi_data_explorations.common import get_annotations, get_vocabulary, load_gc_thredds, add_annotation_qc_flags\n",
    "from ooi_data_explorations.combine_data import combine_datasets\n",
    "from ooi_data_explorations.uncabled.process_flord import flord_datalogger, flord_instrument\n",
    "from ooi_data_explorations.qartod.qc_processing import identify_blocks, create_annotations, process_gross_range, \\\n",
    "    process_climatology, woa_standard_bins, inputs, ANNO_HEADER, CLM_HEADER, GR_HEADER#### Import plotting and visualization tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718334b9",
   "metadata": {},
   "source": [
    "#### Import plotting and visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d626cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39acab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../qartod/\")\n",
    "from qartod.plotting import plot_climatology, plot_gross_range, plot_data_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d5d48f",
   "metadata": {},
   "source": [
    "---\n",
    "## Identify Data Streams\n",
    "This section is necessary to identify all of the data stream associated with a specific instrument. This can be done by querying UFrame and iteratively walking through all of the API endpoints. The results are saved into a csv file so this step doesn't have to be repeated each time.\n",
    "\n",
    "First, set the instrument to search for using OOI terminology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument = \"FLOR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb21299",
   "metadata": {},
   "source": [
    "### Query OOINet for Data Streams <br>\n",
    "If the data streams for a given instrument have not yet been identified from OOINet, then want to query OOINet for the data sets and save them to the local memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    datasets = pd.read_csv(f\"../data/{instrument}_datasets.csv\")\n",
    "except:\n",
    "    datasets = OOINet.search_datasets(instrument=instrument, English_names=True)\n",
    "    datasets.to_csv(f\"../data/{instrument}_datasets.csv\", index=False)\n",
    "datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de8866",
   "metadata": {},
   "source": [
    "Separate out the CGSN datasets from the EA and RCA datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d330ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgsn = datasets[\"array\"].apply(lambda x: True if x.startswith((\"CP\",\"GA\",\"GI\",\"GP\",\"GS\")) else False)\n",
    "datasets = datasets[cgsn]\n",
    "flord = datasets[\"refdes\"].apply(lambda x: True if \"FLORD\" in x else False)\n",
    "datasets = datasets[flord]\n",
    "datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b415c3",
   "metadata": {},
   "source": [
    "Drop all of the vehicle reference designators (those with \"MOAS\" in them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7258ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "moas = datasets[\"array\"].apply(lambda x: True if \"MOAS\" in x else False)\n",
    "datasets = datasets[~moas]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fba0bac",
   "metadata": {},
   "source": [
    "---\n",
    "## Single Reference Designator\n",
    "The reference designator acts as a key for an instrument located at a specific location. First, select a reference designator (refdes) to request data from OOINet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d9a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_designators = sorted(datasets[\"refdes\"])\n",
    "print(\"Number of reference designators: \" + str(len(reference_designators)))\n",
    "for refdes in reference_designators:\n",
    "    print(refdes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b8ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "refdes = reference_designators[k]\n",
    "print(refdes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4dba4a",
   "metadata": {},
   "source": [
    "#### Sensor Vocab\n",
    "The vocab provides information about the instrument model and type, its location (with descriptive names), depth, and manufacturer. Get the vocab for the given reference designator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29808548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = M2M.get_vocab(refdes)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e9503",
   "metadata": {},
   "source": [
    "#### Sensor Deployments\n",
    "Download the deployment information for the selected reference designator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23375191",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments = M2M.get_deployments(refdes)\n",
    "deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b8c20a",
   "metadata": {},
   "source": [
    "#### Sensor Data Streams\n",
    "Next, select the specific data streams for the given reference designator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastreams = M2M.get_datastreams(refdes)\n",
    "datastreams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb4b9e4",
   "metadata": {},
   "source": [
    "---\n",
    "## Metadata \n",
    "The metadata contains the following important key pieces of data for each reference designator: **method**, **stream**, **particleKey**, and **count**. The method and stream are necessary for identifying and loading the relevant dataset. The particleKey tells us which data variables in the dataset we should be calculating the QARTOD parameters for. The count lets us know which dataset (the recovered instrument, recovered host, or telemetered) contains the most data and likely has the best record to use to calculate the QARTOD tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ac4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = M2M.get_metadata(refdes)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a4e3e",
   "metadata": {},
   "source": [
    "#### Sensor Parameters\n",
    "Each instrument returns multiple parameters containing a variety of low-level instrument output and metadata. However, we are interested in science-relevant parameters for calculating the relevant QARTOD test limits. We can identify the science parameters based on the preload database, which designates the science parameters with a \"data level\" of L1 or L2. \n",
    "\n",
    "Consequently, we through several steps to identify the relevant parameters. First, we query the preload database with the relevant metadata for a reference designator. Then, we filter the metadata for the science-relevant data streams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da2e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_science_parameters(metadata):\n",
    "    \"\"\"This function returns the science parameters for each datastream\"\"\"\n",
    "    \n",
    "    def filter_parameter_ids(pdId, pid_dict):\n",
    "        data_level = pid_dict.get(pdId)\n",
    "        if data_level is not None:\n",
    "            if data_level > 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # Filter the parameters for processed science parameters\n",
    "    data_levels = M2M.get_parameter_data_levels(metadata)\n",
    "    mask = metadata[\"pdId\"].apply(lambda x: filter_parameter_ids(x, data_levels))\n",
    "    metadata = metadata[mask]\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def filter_metadata(metadata):\n",
    "    science_vars = filter_science_parameters(metadata)\n",
    "    # Next, eliminate the optode temperature from the stream\n",
    "    mask = science_vars[\"particleKey\"].apply(lambda x: False if \"temp\" in x else True)\n",
    "    science_vars = science_vars[mask]\n",
    "    science_vars = science_vars.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "    science_vars = science_vars.reset_index()\n",
    "    science_vars = science_vars.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "    science_vars = science_vars.explode(column=\"particleKey\")\n",
    "    return science_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b00874",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_vars = filter_science_parameters(metadata)\n",
    "science_vars = science_vars.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "science_vars = science_vars.reset_index()\n",
    "science_vars = science_vars.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "science_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57b9603",
   "metadata": {},
   "source": [
    "---\n",
    "## Load the Data\n",
    "Next, load the data from the different delivery methods into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc2d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_overlaps(ds, deployments):\n",
    "    \"\"\"Trim overlapping deployment data (necessary to use xr.open_mfdataset)\"\"\"\n",
    "    # --------------------------------\n",
    "    # Second, get the deployment times\n",
    "    deployments = deployments.sort_values(by=\"deploymentNumber\")\n",
    "    deployments = deployments.set_index(keys=\"deploymentNumber\")\n",
    "    # Shift the start times by (-1) \n",
    "    deployEnd = deployments[\"deployStart\"].shift(-1)\n",
    "    # Find where the deployEnd times are earlier than the deployStart times\n",
    "    mask = deployments[\"deployEnd\"] > deployEnd\n",
    "    # Wherever the deployEnd times occur after the shifted deployStart times, replace those deployEnd times\n",
    "    deployments[\"deployEnd\"][mask] = deployEnd[mask]\n",
    "    deployments[\"deployEnd\"] = deployments[\"deployEnd\"].apply(lambda x: pd.to_datetime(x))\n",
    "    \n",
    "    # ---------------------------------\n",
    "    # With the deployments info, can write a preprocess function to filter \n",
    "    # the data based on the deployment number\n",
    "    depNum = np.unique(ds[\"deployment\"])\n",
    "    deployInfo = deployments.loc[depNum]\n",
    "    deployStart = deployInfo[\"deployStart\"].values[0]\n",
    "    deployEnd = deployInfo[\"deployEnd\"].values[0]\n",
    "    \n",
    "    # Select the dataset data which falls within the specified time range\n",
    "    ds = ds.sel(time=slice(deployStart, deployEnd))\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40042ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datalogger(ds):\n",
    "    ds = process_file(ds)\n",
    "    ds = trim_overlaps(ds, deployments)\n",
    "    ds = flord_datalogger(ds)\n",
    "    gc.collect()\n",
    "    return ds\n",
    "\n",
    "def preprocess_instrument(ds):\n",
    "    ds = process_file(ds)\n",
    "    ds = trim_overlaps(ds, deployments)\n",
    "    ds = flord_instrument(ds)\n",
    "    gc.collect()\n",
    "    return ds\n",
    "\n",
    "def preprocess_wfp(ds):\n",
    "    ds = process_file(ds)\n",
    "    ds = trim_overlaps(ds, deployments)\n",
    "    ds = flord_wfp(ds)\n",
    "    gc.collect()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39337fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the \"metadata\" datastreams; use only the regular dataset\n",
    "mask = datastreams[\"stream\"].apply(lambda x: False if \"metadata\" in x or \"blank\" in x else True)\n",
    "datastreams = datastreams[mask]\n",
    "datastreams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ffec7",
   "metadata": {},
   "source": [
    "---\n",
    "## Download Data\n",
    "To access data, there are two applicable methods. The first is to download the data and save the netCDF files locally. The second is to access and process the files remotely on the THREDDS server, without having to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89686139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the available datasets\n",
    "for index in datastreams.index:\n",
    "    # Get the method and stream\n",
    "    method = datastreams.loc[index][\"method\"]\n",
    "    stream = datastreams.loc[index][\"stream\"]\n",
    "\n",
    "    # Get the URL - first try the goldCopy thredds server\n",
    "    thredds_url = M2M.get_thredds_url(refdes, method, stream, goldCopy=True)\n",
    "\n",
    "    # Get the catalog\n",
    "    catalog = M2M.get_thredds_catalog(thredds_url)\n",
    "\n",
    "    # Clean the catalog\n",
    "    catalog = M2M.clean_catalog(catalog, stream, deployments)\n",
    "    \n",
    "    # Get the links to the THREDDs server and load the data\n",
    "    dodsC = M2M.URLS[\"goldCopy_dodsC\"]\n",
    "    \n",
    "    # Not all datasets have made it into the goldCopy THREDDS - in that case, need to request\n",
    "    # from OOINet\n",
    "    if len(catalog) == 0:\n",
    "        # Get the URL - first try the goldCopy thredds server\n",
    "        thredds_url = M2M.get_thredds_url(refdes, method, stream, goldCopy=False)\n",
    "\n",
    "        # Get the catalog\n",
    "        catalog = M2M.get_thredds_catalog(thredds_url)\n",
    "\n",
    "        # Clean the catalog\n",
    "        catalog = M2M.clean_catalog(catalog, stream, deployments)\n",
    "\n",
    "        # Get the links to the THREDDs server and load the data\n",
    "        dodsC = M2M.URLS[\"dodsC\"]\n",
    "    \n",
    "    # Now load the data\n",
    "    if method == \"telemetered\":\n",
    "        tele_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        if \"WFP\" in refdes:\n",
    "            with ProgressBar():\n",
    "                tele_data = xr.open_mfdataset(tele_files, preprocess=preprocess_wfp, parallel=True)\n",
    "        else:\n",
    "            with ProgressBar():\n",
    "                tele_data = xr.open_mfdataset(tele_files, preprocess=preprocess_datalogger, parallel=True)\n",
    "    elif method == \"recovered_host\":\n",
    "        host_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        with ProgressBar():\n",
    "            host_data = xr.open_mfdataset(host_files, preprocess=preprocess_datalogger, parallel=True)\n",
    "    elif method == \"recovered_inst\":\n",
    "        inst_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        with ProgressBar():\n",
    "            inst_data = xr.open_mfdataset(inst_files, preprocess=preprocess_instrument, parallel=True)\n",
    "    elif method == \"recovered_wfp\":\n",
    "        wfp_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        wfp_files = [f for f in wfp_files if \"blank\" not in f]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        with ProgressBar():\n",
    "            host_data = xr.open_mfdataset(wfp_files, preprocess=preprocess_wfp, parallel=True)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd89c20",
   "metadata": {},
   "source": [
    "#### Combine the datasets into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da177a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combine_datasets(tele_data, host_data, None, None)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a5f7d2",
   "metadata": {},
   "source": [
    "**Clean up workspace variables and free up memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c375b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_data.close()\n",
    "tele_data.close()\n",
    "del tele_data, host_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0bd84",
   "metadata": {},
   "source": [
    "---\n",
    "## Process the FLORD\n",
    "The FLORD data needs extensive reprocessing before we can generate the qartod tables. So we download the data, reprocess it, and then will use the reprocessed data to calculate the statistics for the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc7fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hitl_qc_flags(data, refdes, annotations):\n",
    "    site, node, sensor = refdes.split(\"-\",2)\n",
    "    # create boolean arrays of the data marked as \"fail\" by the quality checks and generate initial\n",
    "    # HITL annotations that can be combined with system annotations to create a cleaned up data set\n",
    "    # prior to calculating the QARTOD test values\n",
    "    if node == 'WFP01':\n",
    "        index = 10  # decimate the WFP data so we can process it\n",
    "    else:\n",
    "        index = 1\n",
    "    if \"estimated_chlorophyll_qc_summary_flag\" in data.variables:\n",
    "        chl_fail = data.estimated_chlorophyll_qc_summary_flag.where(data.estimated_chlorophyll_qc_summary_flag > 3).notnull()\n",
    "        blocks = identify_blocks(chl_fail[::index], [18, 72])\n",
    "        chl_hitl = create_annotations(site, node, sensor, blocks)\n",
    "        chl_hitl['parameters'] = [[22, 1141] for i in chl_hitl['parameters']]\n",
    "    else:\n",
    "        chl_hitl = None\n",
    "\n",
    "    if \"fluorometric_cdom_qc_summary_flag\" in data.variables:\n",
    "        cdom_fail = data.fluorometric_cdom_qc_summary_flag.where(data.fluorometric_cdom_qc_summary_flag > 3).notnull()\n",
    "        blocks = identify_blocks(cdom_fail[::index], [18, 72])\n",
    "        cdom_hitl = create_annotations(site, node, sensor, blocks)\n",
    "        cdom_hitl['parameters'] = [[23, 1143] for i in cdom_hitl['parameters']]\n",
    "    else:\n",
    "        cdom_hitl = None\n",
    "\n",
    "    if \"beta_700_qc_summary_flag\" in data.variables:\n",
    "        beta_fail = data.beta_700_qc_summary_flag.where(data.beta_700_qc_summary_flag > 3).notnull()\n",
    "        blocks = identify_blocks(beta_fail[::index], [18, 72], 24)\n",
    "        beta_hitl = create_annotations(site, node, sensor, blocks)\n",
    "        beta_hitl['parameters'] = [[24, 25, 1139] for i in beta_hitl['parameters']]\n",
    "    else:\n",
    "        beta_hitl = None\n",
    "\n",
    "    # combine the different dictionaries into a single HITL annotation dictionary for later use\n",
    "    hitl = chl_hitl.copy()\n",
    "    for d in (cdom_hitl, beta_hitl):\n",
    "        if d is None:\n",
    "            pass\n",
    "        else:\n",
    "            for key, value in d.items():\n",
    "                hitl[key] = hitl[key] + d[key]\n",
    "\n",
    "    # get the current system annotations for the sensor\n",
    "    annotations = pd.DataFrame(annotations)\n",
    "    if not annotations.empty:\n",
    "        annotations = annotations.drop(columns=['@class'])\n",
    "        annotations['beginDate'] = pd.to_datetime(annotations.beginDT, unit='ms').dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "        annotations['endDate'] = pd.to_datetime(annotations.endDT, unit='ms').dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "    # append the fail annotations to the existing annotations\n",
    "    annotations = annotations.append(pd.DataFrame(hitl), ignore_index=True, sort=False)\n",
    "\n",
    "    # create an annotation-based quality flag\n",
    "    data = add_annotation_qc_flags(data, annotations)\n",
    "\n",
    "    # clean-up the data, NaN-ing values that were marked as fail in the QC checks and/or identified as a block\n",
    "    # of failed data, and then removing all records where the rollup annotation (every parameter fails) was\n",
    "    # set to fail.\n",
    "    if \"estimated_chlorophyll\" in data.variables and chl_fail is not None:\n",
    "        data['estimated_chlorophyll'][chl_fail] = np.nan\n",
    "        if 'fluorometric_chl_a_annotations_qc_results' in data.variables:\n",
    "            m = data.fluorometric_chl_a_annotations_qc_results == 4\n",
    "            data['estimated_chlorophyll'][m] = np.nan\n",
    "\n",
    "    if \"fluorometric_cdom\" in data.variables and cdom_fail is not None:\n",
    "        data['fluorometric_cdom'][cdom_fail] = np.nan\n",
    "        if 'fluorometric_cdom_annotations_qc_results' in data.variables:\n",
    "            m = data.fluorometric_cdom_annotations_qc_results == 4\n",
    "            data['fluorometric_cdom'][m] = np.nan\n",
    "\n",
    "    if \"beta_700\" in data.variables and beta_fail is not None:\n",
    "        data['beta_700'][beta_fail] = np.nan\n",
    "        if 'total_volume_scattering_coefficient_annotations_qc_results' in data.variables:\n",
    "            m = data.total_volume_scattering_coefficient_annotations_qc_results == 4\n",
    "            data['beta_700'][m] = np.nan\n",
    "            if \"bback\" in data.variables:\n",
    "                data['bback'][m] = np.nan\n",
    "\n",
    "    if 'rollup_annotations_qc_results' in data.variables:\n",
    "        data = data.where(data.rollup_annotations_qc_results != 4)\n",
    "        \n",
    "    return data    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7da0a8",
   "metadata": {},
   "source": [
    "#### Annotations\n",
    "The annotations associated with a specific reference designator may contain relevant information on the performance or reliability of the data for a given dataset. The annotations are downloaded from OOINet as a json and processed into a pandas dataframe. Each annotation may apply to the entire dataset, to a specific stream, or to a specific variable. With the downloaed annotations, we can use the information contained in the ```qcFlag``` column to translate the annotations into QC flags, which can then be used to filter out bad data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bfb521",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = M2M.get_annotations(refdes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a06de2",
   "metadata": {},
   "source": [
    "**Add the HITL QC Flags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0864e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_hitl_qc_flags(data, refdes, annotations)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f67aa9",
   "metadata": {},
   "source": [
    "---\n",
    "## Gross Range\n",
    "The Gross Range QARTOD test consists of two parameters: a fail range which indicates when the data is bad, and a suspect range which indicates when data is either questionable or interesting. The fail range values are set based upon the instrument/measurement and associated calibration. For example, the conductivity sensors are calibration for measurements between 0 (freshwater) and 9 (highly-saline waters). The suspect range values are calculated based on the mean of the available data $\\pm$3$\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1079586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.qartod.gross_range import GrossRange\n",
    "from ooi_data_explorations.qartod.plotting import *\n",
    "from ooi_data_explorations.qartod.qc_processing import format_gross_range, format_climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c01c5e",
   "metadata": {},
   "source": [
    "#### Test Parameters & Sensor Ranges\n",
    "Map out the data variables in the data set to the data stream inputs and the associated sensor ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e080c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parameters = {\n",
    "    \"estimated_chlorophyll\": {\n",
    "        \"inp\": \"fluorometric_chlorophyll_a\",\n",
    "        \"sensor_range\": [0, 30],\n",
    "    },\n",
    "    \"beta_700\": {\n",
    "        \"inp\": \"total_volume_scattering_coefficient\",\n",
    "        \"sensor_range\": [0, 5]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8806be75",
   "metadata": {},
   "source": [
    "**Calculate the Gross Range values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a640fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "site, node, sensor = refdes.split(\"-\", 2)\n",
    "gross_range_table = pd.DataFrame()\n",
    "\n",
    "for param in test_parameters:\n",
    "    sensor_range = test_parameters.get(param).get(\"sensor_range\")\n",
    "    inp = test_parameters.get(param).get(\"inp\") \n",
    "    \n",
    "    if param in data.variables:\n",
    "        print(f\"##### Calculating gross range for {param} #####\")\n",
    "        # Check if there is enough data\n",
    "        if len(data[param].dropna(dim=\"time\")) < 100:\n",
    "            user_range = sensor_range\n",
    "            source = \"Not enough data to calculate user range.\"\n",
    "        else:\n",
    "            gross_range = GrossRange(sensor_range[0], sensor_range[1])\n",
    "            gross_range.fit(data, param, check_normality=True)\n",
    "            user_range = [gross_range.suspect_min, gross_range.suspect_max]\n",
    "            source = gross_range.source\n",
    "        # Check which streams have the param in it\n",
    "        streams = metadata[metadata[\"particleKey\"] == inp][\"stream\"].unique()\n",
    "        for stream in streams:\n",
    "            qc_dict = format_gross_range(inp, sensor_range, user_range, site, node, sensor, stream, source)\n",
    "            gross_range_table = gross_range_table.append(qc_dict, ignore_index=True)\n",
    "            \n",
    "        # ------------------ Plot the gross range ------------------\n",
    "        if data[param].time.size > 100000:\n",
    "            try:\n",
    "                subset = sorted(np.random.choice(data.time, 100000, replace=False))\n",
    "                subset_data = data.sel(time=subset)\n",
    "                plot_gross_range(subset_data, param, gross_range)\n",
    "                del subset, subset_data\n",
    "                gc.collect()\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try:\n",
    "                plot_gross_range(data, param, gross_range**Calculate the Gross Range values**)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6117d06e",
   "metadata": {},
   "source": [
    "**Add the stream name and the source comment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0487f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_range_table['notes'] = ('User range based on data collected through {}.'.format(\"2021-01-01\"))\n",
    "gross_range_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148f875",
   "metadata": {},
   "source": [
    "**Check the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb81798",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in gross_range_table.index:\n",
    "    print(gross_range_table.loc[ind][\"qcConfig\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28ea83",
   "metadata": {},
   "source": [
    "**Save the gross range table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c127c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "gross_range_table.to_csv(f\"../results/gross_range/{refdes}.csv\", index=False, columns=GR_HEADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ce22b",
   "metadata": {},
   "source": [
    "---\n",
    "## Climatology\n",
    "For the climatology QARTOD test, First, we bin the data by month and take the mean. The binned-montly means are then fit with a 2 cycle harmonic via Ordinary-Least-Squares (OLS) regression. Ranges are calculated based on the 3$\\sigma$ calculated from the OLS-fitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb1c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.qartod.climatology import Climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_climatology_table(ds, param, tinp, zinp, sensor_range, depth_bins):\n",
    "    \"\"\"Function which calculates the climatology table based on the \"\"\"\n",
    "    \n",
    "    climatologyTable = pd.DataFrame()\n",
    "    \n",
    "    if depth_bins is None:\n",
    "        # Filter out the data outside the sensor range\n",
    "        m = (ds[param] > sensor_range[0]) & (ds[param] < sensor_range[1]) & (~np.isnan(ds[param]))\n",
    "        param_data = ds[param][m]\n",
    "        \n",
    "        # Fit the climatology for the selected data\n",
    "        pmin, pmax = [0, 0]\n",
    "        \n",
    "        try:\n",
    "            climatology = Climatology()\n",
    "            climatology.fit(param_data)\n",
    "\n",
    "            # Create the depth index\n",
    "            zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "            # Create the monthly bins\n",
    "            tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "            # Calculate the climatology data\n",
    "            vmin = climatology.monthly_fit - climatology.monthly_std*3\n",
    "            vmin = np.floor(vmin*10000)/10000\n",
    "            for vind in vmin.index:\n",
    "                if vmin[vind] < sensor_range[0] or vmin[vind] > sensor_range[1]:\n",
    "                    vmin[vind] = sensor_range[0]\n",
    "            vmax = climatology.monthly_fit + climatology.monthly_std*3\n",
    "            for vind in vmax.index:\n",
    "                if vmax[vind] < sensor_range[0] or vmax[vind] > sensor_range[1]:\n",
    "                    vmax[vind] = sensor_range[1]\n",
    "            vmax = np.floor(vmax*10000)/10000\n",
    "            vdata = pd.Series(data=zip(vmin, vmax), index=vmin.index).apply(lambda x: [v for v in x])\n",
    "            vspan = vdata.values.reshape(1,-1)\n",
    "\n",
    "            # Build the climatology dataframe\n",
    "            climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "        except:\n",
    "            # Here is where to create nans if insufficient data to fit\n",
    "            # Create the depth index\n",
    "            zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "            # Create the monthly bins\n",
    "            tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "            # Create a series filled with nans\n",
    "            vals = []\n",
    "            for i in np.arange(len(tspan)):\n",
    "                vals.append([np.nan, np.nan])\n",
    "            vspan = pd.Series(data=vals, index=tspan).values.reshape(1,-1)\n",
    "\n",
    "            # Add to the data\n",
    "            climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "            \n",
    "        del ds, vspan, tspan, zspan\n",
    "        gc.collect()\n",
    "        \n",
    "    else:        \n",
    "    # Iterate through the depth bins to calculate the climatology for each depth bin\n",
    "        for dbin in depth_bins:\n",
    "            # Get the pressure range to bin from\n",
    "            pmin, pmax = dbin[0], dbin[1]\n",
    "\n",
    "            # Select the data from the pressure range\n",
    "            bin_data = data.where((data[zinp] >= pmin) & (data[zinp] <= pmax), drop=True)\n",
    "\n",
    "            # sort based on time and make sure we have a monotonic dataset\n",
    "            bin_data = bin_data.sortby('time')\n",
    "            _, index = np.unique(bin_data['time'], return_index=True)\n",
    "            bin_data = bin_data.isel(time=index)\n",
    "\n",
    "            # Filter out the data outside the sensor range\n",
    "            m = (bin_data[param] > sensor_range[0]) & (bin_data[param] < sensor_range[1]) & (~np.isnan(bin_data[param]))\n",
    "            param_data = bin_data[param][m]\n",
    "\n",
    "            # Fit the climatology for the selected data\n",
    "            try:\n",
    "                climatology = Climatology()\n",
    "                climatology.fit(param_data)\n",
    "\n",
    "                # Create the depth index\n",
    "                zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "                # Create the monthly bins\n",
    "                tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "                # Calculate the climatology data\n",
    "                vmin = climatology.monthly_fit - climatology.monthly_std*3\n",
    "                vmin = np.floor(vmin*10000)/10000\n",
    "                for vind in vmin.index:\n",
    "                    if vmin[vind] < sensor_range[0] or vmin[vind] > sensor_range[1]:\n",
    "                        vmin[vind] = sensor_range[0]\n",
    "                vmax = climatology.monthly_fit + climatology.monthly_std*3\n",
    "                vmax = np.floor(vmax*10000)/10000\n",
    "                for vind in vmax.index:\n",
    "                    if vmax[vind] < sensor_range[0] or vmax[vind] > sensor_range[1]:\n",
    "                        vmax[vind] = sensor_range[1]\n",
    "                vdata = pd.Series(data=zip(vmin, vmax), index=vmin.index).apply(lambda x: [v for v in x])\n",
    "                vspan = vdata.values.reshape(1,-1)\n",
    "\n",
    "                # Build the climatology dataframe\n",
    "                climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "            except:\n",
    "                # Here is where to create nans if insufficient data to fit\n",
    "                # Create the depth index\n",
    "                zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "                # Create the monthly bins\n",
    "                tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "                # Create a series filled with nans\n",
    "                vals = []\n",
    "                for i in np.arange(len(tspan)):\n",
    "                    vals.append([np.nan, np.nan])\n",
    "                vspan = pd.Series(data=vals, index=tspan).values.reshape(1,-1)\n",
    "\n",
    "                # Add to the data\n",
    "                climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "            del climatology, bin_data, vspan, tspan, zspan\n",
    "            gc.collect()\n",
    "    \n",
    "    return climatologyTable, climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feffff9",
   "metadata": {},
   "source": [
    "**Get the depth bins and filter based on max depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7756e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"WFP\" in refdes:\n",
    "    depth_bins = woa_standard_bins()\n",
    "    pmax = data[\"depth\"].max().values\n",
    "    pmin = data[\"depth\"].min().values\n",
    "    mask = (depth_bins[:, 0] < pmax) | ((depth_bins[:, 0] < pmax) & (depth_bins[:, 1] > pmax)) | (depth_bins[:, 1] < pmin)\n",
    "    depth_bins = depth_bins[mask]\n",
    "else:\n",
    "    depth_bins = None\n",
    "    \n",
    "depth_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3eafc",
   "metadata": {},
   "source": [
    "**Calculate the climatology tables and associated lookup table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163908cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the climatology lookup table\n",
    "climatologyLookup = pd.DataFrame()\n",
    "\n",
    "# Setup the Table Header\n",
    "TBL_HEADER = [\"[1,1]\",\"[2,2]\",\"[3,3]\",\"[4,4]\",\"[5,5]\",\"[6,6]\",\"[7,7]\",\"[8,8]\",\"[9,9]\",\"[10,10]\",\"[11,11]\",\"[12,12]\"]\n",
    "\n",
    "# Set the subsite-node-sensor\n",
    "subsite, node, sensor = refdes.split(\"-\", 2)\n",
    "\n",
    "# Iterate through the parameters\n",
    "for param in parameters:\n",
    "    if param in data.variables:\n",
    "        # ----------------- Depth tables ---------------------\n",
    "        # Get the sensor range of the parameter to test\n",
    "        print(f\"##### Calculating climatology for {param} #####\")\n",
    "        sensor_range = parameters.get(param)\n",
    "        \n",
    "        # Generate the climatology table with the depth bins\n",
    "        climatologyTable, climatology = make_climatology_table(data, param, \"time\", \"depth\", sensor_range, depth_bins)\n",
    "        \n",
    "        # Get the variance and generate the source\n",
    "        if \"WFP\" in refdes:\n",
    "            source = \"Climatology values are calculated from and applicable to standard depth bins.\"\n",
    "        else:\n",
    "            try:\n",
    "                variance = float(np.round(climatology.regression['variance_explained']*100, 1))\n",
    "            except:\n",
    "                variance = 0.0\n",
    "            source = f\"The variance explained by the climatology mode is {variance}%\"\n",
    "\n",
    "        # Create the tableName\n",
    "        tableName = f\"{refdes}-{param}.csv\"\n",
    "        \n",
    "        # Save the results\n",
    "        climatologyTable.to_csv(f\"../results/climatology/climatology_tables/{tableName}\", header=TBL_HEADER)\n",
    "        \n",
    "        # ------------------ Lookup tables ------------------\n",
    "        # Check which streams have the param in it\n",
    "        streams = metadata[metadata[\"particleKey\"] == inp][\"stream\"].unique()\n",
    "        for stream in streams:\n",
    "            qc_dict = {\n",
    "                \"subsite\": subsite,\n",
    "                \"node\": node,\n",
    "                \"sensor\": sensor,\n",
    "                \"stream\": stream,\n",
    "                \"parameters\": {\n",
    "                    \"inp\": inp,\n",
    "                    \"tinp\": \"time\",\n",
    "                    \"zinp\": \"depth\",\n",
    "                },\n",
    "                \"climatologyTable\": f\"climatology_tables/{refdes}-{param}.csv\",\n",
    "                \"source\": source,\n",
    "                \"notes\": \"Climatology based on available data through 2021-01-01.\"\n",
    "            }\n",
    "            # Append to the lookup table\n",
    "            climatologyLookup = climatologyLookup.append(qc_dict, ignore_index=True)\n",
    "            \n",
    "        # ------------------ Plot the climatology ------------------\n",
    "        # Don't attempt to plot the WFP data\n",
    "        if \"WFP\" in refdes:\n",
    "            continue\n",
    "        else:\n",
    "            if data[param].time.size > 100000:\n",
    "                try:\n",
    "                    subset = sorted(np.random.choice(data.time, 100000, replace=False))\n",
    "                    subset_data = data.sel(time=subset)\n",
    "                    plot_climatology(subset_data, param, climatology)\n",
    "                    del subset, subset_data\n",
    "                    gc.collect()\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                try:\n",
    "                    plot_climatology(data, param, climatology)\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4374d4e1",
   "metadata": {},
   "source": [
    "**Check the last climatologyTable for reasonableness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dca8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "climatologyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8828878e",
   "metadata": {},
   "source": [
    "**Check the climatologyLookup table that all the entries made it in**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e7141",
   "metadata": {},
   "outputs": [],
   "source": [
    "climatologyLookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3e8bb",
   "metadata": {},
   "source": [
    "**Save the climatologyLookup table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e55e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "climatologyLookup.to_csv(f\"../results/climatology/{refdes}.csv\", index=False, columns=CLM_HEADER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
